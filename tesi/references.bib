@book{Tanenbaum2006,
    title={Architettura dei calcolatori. Un approccio strutturale},
    author={Andrew Stuart Tanenbaum},
    editor={Ottavio M. D'Antona},
    year={2006},
    pages={626-628},
    publisher={Pearson Education},
    address={Milano},
    isbn={978-8-8719-2271-3},
}

@online{WLCG2023,
    author = {CERN},
    title = {Worldwide LHC Computing Grid},
    year = 2023,
    url = {https://wlcg.web.cern.ch},
    urldate = {2023-10-28}
}

@article{dalpra2019,
    author = {Dal Pra, Stefano and Falabella, Antonio and Fattibene, Enrico and Cincinelli, Gianluca and Magnani, Matteo and Cristofaro, Tiziano and Ruini, Martin},
    year = {2019},
    month = {01},
    pages = {08033},
    title = {Evolution of monitoring, accounting and alerting services at INFN-CNAF Tier-1},
    volume = {214},
    journal = {EPJ Web of Conferences},
    doi = {10.1051/epjconf/201921408033}
}

@misc{hepix2022,
    author = {Andrea Rendina},
    title = {INFN-T1 site report},
    howpublished = {\url{https://indico.cern.ch/event/1200682/contributions/5087586/attachments/2538178/4368754/20221031_InfnT1_site_report.pdf}},
    year = {2022},
    note = {Visitato il 28/10/2023},
}

@online{cnaf_calcolo,
    author = {CNAF},
    title = {WLCG Tier-1 data center - Calcolo},
    url = {https://www.cnaf.infn.it/calcolo/},
    urldate = {2023-10-28}
}

@article{Bortolotti2012,
    doi = {10.1088/1742-6596/396/4/042016},
    url = {https://dx.doi.org/10.1088/1742-6596/396/4/042016},
    year = {2012},
    month = {dec},
    publisher = {},
    volume = {396},
    number = {4},
    pages = {042016},
    author = {G Bortolotti and A Cavalli and L Chiarelli and A Chierici and S Dal Pra and L dell'Agnello and D De Girolamo and M Donatelli and A Ferraro and D Gregori and A Italiano and B Martelli and A Mazza and M Onofri and A Prosperini and P P Ricci and E Ronchieri and F Rosso and V Sapunenko and R Veraldi and C Vistoli S Zani},
    title = {The INFN Tier-1},
    journal = {Journal of Physics: Conference Series},
    abstract = {INFN-CNAF is the central computing facility of INFN: it is the Italian Tier-1 for the experiments at LHC, but also one of the main Italian computing facilities for several other experiments such as BABAR, CDF, SuperB, Virgo, Argo, AMS, Pamela, MAGIC, Auger etc. Currently there is an installed CPU capacity of 100,000 HS06, a net disk capacity of 9 PB and an equivalent amount of tape storage (these figures are going to be increased in the first half of 2012 respectively to 125,000 HS06, 12 PB and 18 PB). More than 80,000 computing jobs are executed daily on the farm, managed by LSF, accessing the storage, managed by GPFS, with an aggregate bandwidth up to several GB/s. The access to the storage system from the farm is direct through the file protocol. The interconnection of the computing resources and the data storage is based on 10 Gbps technology. The disk-servers and the storage systems are connected through a Storage Area Network allowing a complete flexibility and easiness of management; dedicated disk-servers are connected, also via the SAN, to the tape library. The INFN Tier-1 is connected to the other centers via 3×10 Gbps links (to be upgraded at the end of 2012), including the LHCOPN and to the LHCONE. In this paper we show the main results of our center after 2 full years of run of LHC.}
}

@article{Moore1965,
    author = {Moore, Gordon E.},
    title = {Cramming more components onto integrated circuits},
    journal = {Electronics},
    volume = {38},
    number = {8},
    pages = {114-117},
    year = {1965},
    publisher = {McGraw-Hill}
}

@article{Shalf2015,
    author = {Shalf, J. M. and Leland, R.},
    journal = {Computer},
    title = {Computing Beyond Moore's Law},
    year = {2015},
    volume = {48},
    number = {12},
    issn = {1558-0814},
    pages = {14--23},
    abstract = {Photolithography systems are on pace to reach atomic scale by the mid-2020s, necessitating alternatives to continue realizing faster, more predictable, and cheaper computing performance. If the end of Moore's law is real, a research agenda is needed to assess the viability of novel semiconductor technologies and navigate the ensuing challenges.},
    keywords = {CMOS integrated circuits; CMOS technology; Moore's law; consumer electronics; biological system modeling; computational modeling},
    doi = {10.1109/MC.2015.374},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = dec
}

@article{Theis2017,
    author = {Theis, Thomas N. and Wong, H.-S. Philip},
    journal = {Computing in Science \& Engineering},
    title = {The End of Moore's Law: A New Beginning for Information Technology},
    year = {2017},
    volume = {19},
    number = {2},
    pages = {41--50},
    doi = {10.1109/MCSE.2017.29}
}

@inproceedings{Villa2014,
    author={Villa, Oreste and Johnson, Daniel R. and Oconnor, Mike and Bolotin, Evgeny and Nellans, David and Luitjens, Justin and Sakharnykh, Nikolai and Wang, Peng and Micikevicius, Paulius and Scudiero, Anthony and Keckler, Stephen W. and Dally, William J.},
    booktitle={SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
    title={Scaling the Power Wall: A Path to Exascale}, 
    year={2014},
    volume={},
    number={},
    pages={830-841},
    doi={10.1109/SC.2014.73}
}

@article{Cappello2014, 
    title={Toward Exascale Resilience: 2014 update}, 
    volume={1}, 
    url={https://superfri.org/index.php/superfri/article/view/14}, 
    DOI={10.14529/jsfi140101}, 
    abstract={Resilience is a major roadblock for HPC executions on future
    exascale systems. These systems will typically gather millions of CPU cores
    running up to a billion threads. Projections from current large systems and
    technology evolution predict errors will happen in exascale systems many
    times per day. These errors will propagate and generate various kinds of
    malfunctions, from simple process crashes to result corruptions. The past
    five years have seen extraordinary technical progress in many domains
    related to exascale resilience. Several technical options, initially
    considered inapplicable or unrealistic in the HPC context, have
    demonstrated surprising successes. Despite this progress, the exascale
    resilience problem is not solved, and the community is still facing the
    difficult challenge of ensuring that exascale applications complete and
    generate correct results while running on unstable systems. Since 2009,
    many workshops, studies, and reports have improved the definition of the
    resilience problem and provided refined recommendations. Some projections
    made during the previous decades and some priorities established from these
    projections need to be revised. This paper surveys what the community has
    learned in the past five years and summarizes the research problems still
    considered critical by the HPC community.}, 
    number={1},
    journal={Supercomputing Frontiers and Innovations}, 
    author={Cappello, Franck and Geist, Al and Gropp, William and Kale, Sanjay
    and Kramer, Bill and Snir, Marc}, 
    year={2014}, 
    month={Jun},
    pages={5–28} 
}

@article{vanderMaaten2008,
    title={Visualizing Data using t-SNE},
    author={van der Maaten, Laurens and Hinton, Geoffrey},
    journal={Journal of Machine Learning Research},
    volume={9},
    year={2008},
    pages={2579--2605},
    url={https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf}
}

@article{Pezzotti2017,
    author={Pezzotti, Nicola and Lelieveldt, Boudewijn P. F. and Maaten, Laurens van der and Höllt, Thomas and Eisemann, Elmar and Vilanova, Anna},
    journal={IEEE Transactions on Visualization and Computer Graphics}, 
    title={Approximated and User Steerable tSNE for Progressive Visual Analytics}, 
    year={2017},
    volume={23},
    number={7},
    pages={1739-1752},
    doi={10.1109/TVCG.2016.2570755}
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Wattenberg2016,
    author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
    title = {How to Use t-SNE Effectively},
    journal = {Distill},
    year = {2016},
    url = {http://distill.pub/2016/misread-tsne},
    doi = {10.23915/distill.00002}
}

@article{Banjongkan2021,
    title={A Study of Job Failure Prediction at Job Submit-State and Job Start-State in High-Performance Computing System: Using Decision Tree Algorithms},
    author={Anupong Banjongkan and Watthana Pongsena and Nittaya Kerdprasop and Kittisak Kerdprasop},
    journal={Journal of Advances in Information Technology},
    year={2021},
    volume={12},
    pages={84-92},
    url={https://api.semanticscholar.org/CorpusID:234326555}
}

@Inbook{LeCun2012,
    author="LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert",
    editor="Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert",
    title="Efficient BackProp",
    bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
    year="2012",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="9--48",
    abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.",
    isbn="978-3-642-35289-8",
    doi="10.1007/978-3-642-35289-8_3",
    url="https://doi.org/10.1007/978-3-642-35289-8_3"
}

@book{he2013,
    title={Imbalanced Learning: Foundations, Algorithms, and Applications},
    author={He, H. and Ma, Y.},
    isbn={9781118646335},
    lccn={2013019555},
    url={https://books.google.it/books?id=CVHx-Gp9jzUC},
    year={2013},
    publisher={Wiley}
}

@misc{kingma2022,
    title={Auto-Encoding Variational Bayes}, 
    author={Diederik P Kingma and Max Welling},
    year={2022},
    eprint={1312.6114},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@online{brownlee2021,
    author={Jason Brownlee},
    title={SMOTE for Imbalanced Classification with Python},
    year=2021,
    month={03},
    day={17},
    url={https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/},
    urldate={2023-11-15},
    organization={Machine Learning Mastery}
}

@online{shafkat2018,
    author={Irhum Shafkat},
    title={Intuitively Understanding Variational Autoencoders},
    year=2018,
    month={02},
    day={18},
    url={https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf},
    urldate={2023-11-15},
    organization = {Towards Data Science}
}

@book{fernandez2018,
    author={Fernández, Alberto and García, Salvador and Galar, Mikel and Prati, Ronaldo and Krawczyk, Bartosz and Herrera, Francisco},
    year={2018},
    month={01},
    title={Learning from Imbalanced Data Sets},
    isbn={978-3-319-98073-7},
    doi={10.1007/978-3-319-98074-4}
}

@book{geron2019,
    author={Aurélien Géron},
    title={Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow},
    edition={2},
    publisher={O'Reilly Media},
    year={2019},
}

@article{pimentel2014,
    title = {A review of novelty detection},
    journal = {Signal Processing},
    volume = {99},
    pages = {215-249},
    year = {2014},
    issn = {0165-1684},
    doi = {https://doi.org/10.1016/j.sigpro.2013.12.026},
    url = {https://www.sciencedirect.com/science/article/pii/S016516841300515X},
    author = {Marco A.F. Pimentel and David A. Clifton and Lei Clifton and Lionel Tarassenko},
    keywords = {Novelty detection, One-class classification, Machine learning},
    abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as “one-class classification”, in which a model is constructed to describe “normal” training data. The novelty detection approach is typically used when the quantity of available “abnormal” data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that “normality” may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.}
}

@inproceedings{breiman1997,
    title={Arcing the edge},
    author={L. Breiman},
    year={1997},
    url={https://api.semanticscholar.org/CorpusID:14849468}
}

@article{friedman2001,
    author={Jerome H. Friedman},
    title={{Greedy function approximation: A gradient boosting machine.}},
    volume={29},
    journal={The Annals of Statistics},
    number={5},
    publisher={Institute of Mathematical Statistics},
    pages={1189 -- 1232},
    keywords={boosting, decision trees, Function estimation, robust nonparametric regression},
    year={2001},
    doi={10.1214/aos/1013203451},
    URL={https://doi.org/10.1214/aos/1013203451}
}

@inproceedings{chen2016, 
    series={KDD ’16},
    title={XGBoost: A Scalable Tree Boosting System},
    url={http://dx.doi.org/10.1145/2939672.2939785},
    DOI={10.1145/2939672.2939785},
    booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    publisher={ACM},
    author={Chen, Tianqi and Guestrin, Carlos},
    year={2016},
    month=aug, 
    collection={KDD ’16} 
}

@article{shwartz2021,
    author={Ravid Shwartz{-}Ziv and Amitai Armon},
    title={Tabular Data: Deep Learning is Not All You Need},
    journal={CoRR},
    volume={abs/2106.03253},
    year={2021},
    url={https://arxiv.org/abs/2106.03253},
    eprinttype={arXiv},
    eprint={2106.03253},
    timestamp={Thu, 10 Jun 2021 16:34:18 +0200},
    biburl={https://dblp.org/rec/journals/corr/abs-2106-03253.bib},
    bibsource={dblp computer science bibliography, https://dblp.org}
}

@article{guo2020,
    author={Guo, Rui and Zhao, Zhiqian and Wang, Tao and Liu, Guangheng and Zhao, Jingyi and Gao, Dianrong},
    year={2020},
    month={09},
    pages={6593},
    title={Degradation state recognition of piston pump based on ICEEMDAN and XGBoost},
    volume={10},
    journal={Applied Sciences},
    doi={10.3390/app10186593}
}

@article{lecun1998,
    author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
    journal={Proceedings of the IEEE}, 
    title={Gradient-based learning applied to document recognition}, 
    year={1998},
    volume={86},
    number={11},
    pages={2278-2324},
    doi={10.1109/5.726791}
}

@article{he2020,
    author = {He, Chu and Tu, Mingxia and Xiong, Dehui and Liao, Mingsheng},
    year = {2020},
    month = {02},
    pages = {655},
    title = {Nonlinear Manifold Learning Integrated with Fully Convolutional Networks for PolSAR Image Classification},
    volume = {12},
    journal = {Remote Sensing},
    doi = {10.3390/rs12040655}
}

@misc{he2015,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{hochreiter1997,
    author={Hochreiter, Sepp and Schmidhuber, Jürgen},
    year={1997},
    month={12},
    pages={1735-80},
    title={Long Short-term Memory},
    volume={9},
    journal={Neural computation},
    doi={10.1162/neco.1997.9.8.1735}
}

@online{karpathy2015,
    author={Andrej Karpathy},
    title={The Unreasonable Effectiveness of Recurrent Neural Networks},
    year={2015},
    month={5},
    day={21},
    url={http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
    urldate={2023-11-17},
    organization={Personal Blog of Andrej Karpathy}
}

@misc{wang2016,
    title={Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline}, 
    author={Zhiguang Wang and Weizhong Yan and Tim Oates},
    year={2016},
    eprint={1611.06455},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{long2015,
    title={Fully Convolutional Networks for Semantic Segmentation}, 
    author={Jonathan Long and Evan Shelhamer and Trevor Darrell},
    year={2015},
    eprint={1411.4038},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{vaswani2023,
    title={Attention Is All You Need}, 
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2023},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{borghesi2019,
    title={Anomaly Detection Using Autoencoders in High Performance Computing Systems},
    volume={33},
    ISSN={2159-5399},
    url={http://dx.doi.org/10.1609/aaai.v33i01.33019428},
    DOI={10.1609/aaai.v33i01.33019428},
    number={01},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
    author={Borghesi, Andrea and Bartolini, Andrea and Lombardi, Michele and Milano, Michela and Benini, Luca},
    year={2019},
    month=jul, 
    pages={9428–9433} 
}

@online{raschka2018,
    author={Sebastian Raschka},
    title={Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning - Part IV},
    year={2018},
    url={https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html},
    urldate={2023-11-24},
}

@article{cawley2010,
    author={Cawley, Gavin and Talbot, Nicola},
    year={2010},
    month={07},
    pages={2079-2107},
    title={On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation},
    volume={11},
    journal={Journal of Machine Learning Research}
}

@misc{lin2014,
    title={Network In Network}, 
    author={Min Lin and Qiang Chen and Shuicheng Yan},
    year={2014},
    eprint={1312.4400},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@article{halevy2009,
    author={Halevy, Alon and Norvig, Peter and Pereira, Fernando},
    journal={IEEE Intelligent Systems}, 
    title={The Unreasonable Effectiveness of Data}, 
    year={2009},
    volume={24},
    number={2},
    pages={8-12},
    doi={10.1109/MIS.2009.36}
}

@book{gamma1994,
    title={Design Patterns: Elements of Reusable Object-Oriented Software},
    author={Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
    series={Addison-Wesley Professional Computing Series},
    year={1994},
    publisher={Pearson Education}
}
