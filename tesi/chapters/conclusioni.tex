\label{chap:conclusione}

%Conviene aggiungere un paragrafo all'inizio in cui si ribadisce lo scopo della
%tesi e l'ambito applicativo (identificare i job zombie il prima possibile in
%una farm di calcolo)

Dalle analisi effettuate nel capitolo~\ref{chap:analisi}, sui dati relativi
allo stato dei job in esecuzione e ai dati di accounting, è stato possibile
identificare un sottoinsieme di job che falliscono, denominati ``job zombie''.
La loro prematura identificazione risulta essere particolarmente vantaggiosa
in termini di payoff derivante dalla loro rimozione. Dai risultati ottenuti
nel capitolo~\ref{chap:applicazione}, addestrando un apposito modello
predittivo, è possibile affermare che i job zombie sono identificabili. Sono
stati proposti due modelli (vedi figura~\ref{table:model_results}) che
riescono ad identificare i job zombie su tutti i gruppi di utenti e con dati
recenti. Nonostante i modelli non siano precisi come sperato, e pertanto non
permettano la rimozione automatica dei job, questi consentono di identificare
i job, che con buona probabilità, saranno zombie (1 su 2). I segnali forniti
dal modello possono essere utilizzati per impostare un filtro o un avviso,
permettendo così di controllare manualmente i job sospetti o di stabilire una
regola per la loro eliminazione.

\begin{table}[!ht]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Metrica & Transformer & XGBoost \\ 
        \midrule
        Precisione (classe minoritaria) & 0.42 & 0.57 \\ 
        Recall (classe minoritaria) & 0.55 & 0.42 \\ 
        $F_1$ & 0.73 & 0.74 \\ 
        \bottomrule
    \end{tabular}
    \caption{\small Risultati dei modelli Transformer e XGBoost nell'identificazione
    dei job zombie sul set di test del dataset del periodo 13-31 marzo 2023}
    \label{table:model_results}
\end{table}

Il lavoro può essere proseguito per sviluppi futuri, partendo dai notebook
esistenti disponibili all'indirizzo
\url{https://github.com/alessioarcara/JobFailurePrediction-CNAF}.

Il passo successivo per una ricerca potenziale consiste nell'integrare,
anziché scegliere uno dei due classificatori, entrambi in un ensemble.
Combinando le previsioni di diversi classificatori e applicando una regola di
voto, è possibile compensare sorprendentemente le rispettive debolezze,
ottenendo prestazioni migliorate \cite{geron2019}. Si consiglia, dunque, di
esplorare ulteriori modelli e di integrarli in un ensemble.

Inoltre, l'incremento del numero di campionamenti da un'ora a 24 ore non ha
portato a un corrispondente incremento nella precisione. Tuttavia, questo
potrebbe essere dovuto al fatto di aver considerato più di un gruppo di
utenti. Un'altra ricerca potenziale che potrebbe migliorare le prestazioni è,
invece di considerare tutti i job di tutti i gruppi di utenti per addestrare
un unico modello, addestrare un modello per ogni gruppo di utenti. In questo
modo, si otterrebbe alla fine un array di modelli, che potrebbe mantenere le
stesse prestazioni osservate con il gruppo di utenti ATLAS.

Un altro aspetto importante è la qualità dei dati. Infatti, la qualità e la
quantità dei dati sono più importanti del modello utilizzato
\cite{halevy2009}. È utile arricchire le feature con dati provenienti da altre
fonti. Considerando che il CNAF monitora non solo lo stato dei job, ma anche
quello degli host fisici dove i job sono attualmente in esecuzione, con un
campionamento ogni 3 minuti, un'ulteriore direzione di ricerca potrebbe essere
quella di incrociare questi dati. In questo modo, si potrebbe ottenere
informazioni anche sull'ambiente in cui i job sono in esecuzione.

Sempre in merito ai dati, un'altra ricerca futura potrebbe concentrarsi sul
miglioramento della preparazione dei dati. Questo potrebbe includere
l'aggiunta di metodi alla classe \texttt{Preprocessor} per pulire le serie
storiche, ad esempio rimuovendo gli outlier o applicando una media mobile per
ridurre la variabilità. Inoltre, si potrebbero apportare miglioramenti anche
nell'etichettatura dei dati. Ad esempio, si potrebbe considerare come job
zombie non solo quelli che terminano a causa di un timeout di 7 giorni, ma
anche quelli in cui il \texttt{cputime} rimane invariato per un lungo periodo.
Ciò garantirebbe di identificare con maggiore sicurezza i job zombie, evitando
di classificare erroneamente quei job che richiedono più tempo per completare
i calcoli.
