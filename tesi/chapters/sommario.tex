Il CNAF gestisce uno dei più importanti centri di calcolo in Italia,
utilizzato da gruppi di ricercatori di fisica delle particelle, astrofisica e
altri campi.
Questo centro è dotato di oltre 46000 core distribuiti su 960 host fisici. I
job vengono accodati e schedulati dal sistema batch (HTCondor) attraverso
l'uso di algoritmi di ``fairshare''.
Durante l'esecuzione, vengono monitorate alcune grandezze che vengono
campionate ogni tre minuti e raccolte in un database insieme ai dati di
accounting relativi ai job terminati.
Questo studio esplora l'uso di tecniche di Machine Learning e di Deep Learning
per prevedere il successo o il fallimento dei job, basandosi sull'evoluzione
del loro stato nel tempo.
In particolare, è stato identificato un particolare sottoinsieme di job che
falliscono, denominati zombie. Questi, pur smettendo di effettuare calcoli,
non rilasciano l'host fisico, occupando improduttivamente delle risorse fino
al loro timeout.
L'obiettivo della tesi è stato quello di individuare questi job il più presto
possibile, poiché la loro prematura identificazione risulta essere
particolarmente vantaggiosa in termini di risparmio di risorse derivante dalla
loro rimozione.
Sono stati proposti e validati due modelli capaci di identificare i job che,
con buona probabilità, diventeranno zombie (1 su 2). Le predizioni fornite dal
modello possono essere utilizzate per impostare un filtro o un avviso,
permettendo così di controllare manualmente i job sospetti o di stabilire una
regola per la loro eliminazione.
